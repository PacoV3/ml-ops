{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a366fcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72717930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/08 20:31:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/08 20:31:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a06c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import concat, lit, unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8cf2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption_schema = StructType(fields=[\n",
    "    StructField('Date', StringType()),\n",
    "    StructField('Time', StringType()),\n",
    "    StructField('Global_active_power', DoubleType()),\n",
    "    StructField('Global_reactive_power', DoubleType()),\n",
    "    StructField('Voltage', DoubleType()),\n",
    "    StructField('Global_intensity', DoubleType()),\n",
    "    StructField('Sub_metering_1', DoubleType()),\n",
    "    StructField('Sub_metering_2', DoubleType()),\n",
    "    StructField('Sub_metering_3', DoubleType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c13f8e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                  (0 + 1) / 1]\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile('power-consumption/household-power-consumption-p1.txt')\n",
    "df = spark.read.option('delimiter', ';').option('header', True).schema(consumption_schema).csv(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "783cbe22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: string, Time: string, Global_active_power: double, Global_reactive_power: double, Voltage: double, Global_intensity: double, Sub_metering_1: double, Sub_metering_2: double, Sub_metering_3: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec412ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_df = df.withColumnRenamed('Global_active_power', 'global_active_pow') \\\n",
    "    .withColumnRenamed('Global_reactive_power', 'global_reactive_pow') \\\n",
    "    .withColumnRenamed('Global_intensity', 'global_intesity') \\\n",
    "    .withColumnRenamed('Voltage', 'voltage') \\\n",
    "    .withColumnRenamed('Sub_metering_1', 'sub_metering_1') \\\n",
    "    .withColumnRenamed('Sub_metering_2', 'sub_metering_2') \\\n",
    "    .withColumnRenamed('Sub_metering_3', 'sub_metering_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bd7cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = renamed_df.select(\n",
    "    'global_active_pow', 'global_reactive_pow',\n",
    "    'global_intesity', 'voltage', 'sub_metering_1',\n",
    "    'sub_metering_2', 'sub_metering_3',\n",
    "    unix_timestamp(concat(df.Date, lit(' '), df.Time), 'd/M/yyyy HH:mm:ss').alias('record_timestamp')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdc0e1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+---------------+-------+--------------+--------------+--------------+----------------+\n",
      "|global_active_pow|global_reactive_pow|global_intesity|voltage|sub_metering_1|sub_metering_2|sub_metering_3|record_timestamp|\n",
      "+-----------------+-------------------+---------------+-------+--------------+--------------+--------------+----------------+\n",
      "|            4.216|              0.418|           18.4| 234.84|           0.0|           1.0|          17.0|      1166289840|\n",
      "|             5.36|              0.436|           23.0| 233.63|           0.0|           1.0|          16.0|      1166289900|\n",
      "|            5.374|              0.498|           23.0| 233.29|           0.0|           2.0|          17.0|      1166289960|\n",
      "|            5.388|              0.502|           23.0| 233.74|           0.0|           1.0|          17.0|      1166290020|\n",
      "+-----------------+-------------------+---------------+-------+--------------+--------------+--------------+----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2889f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[global_active_pow: double, global_reactive_pow: double, global_intesity: double, voltage: double, sub_metering_1: double, sub_metering_2: double, sub_metering_3: double, record_timestamp: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "808034cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[max(sub_metering_2): double, max(sub_metering_1): double, max(sub_metering_3): double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df.agg({\"sub_metering_1\": \"max\", \"sub_metering_2\": \"max\", \"sub_metering_3\": \"max\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba1d34e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(max(record_timestamp)=1197418620)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df.agg({\"record_timestamp\": \"max\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf002131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
